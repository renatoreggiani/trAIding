{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.thepythoncode.com/article/stock-price-prediction-in-python-using-tensorflow-2-and-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "    !pip install requests_html\n",
    "    !pip install wrapt --upgrade --ignore-installed\n",
    "    !pip install tensorflow\n",
    "    !pip3 install pandas numpy matplotlib yahoo_fin sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning - Certain functionality \n",
      "             requires requests_html, which is not installed.\n",
      "             \n",
      "             Install using: \n",
      "             pip install requests_html\n",
      "             \n",
      "             After installation, you may have to restart your Python session.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import randomb\n",
    "\n",
    "from functions import get_finance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed, so we can get the same results after rerunning several times\n",
    "np.random.seed(314)\n",
    "tf.random.set_seed(314)\n",
    "random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(sequence_length, n_features, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), batch_input_shape=(None, sequence_length, n_features)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, batch_input_shape=(None, sequence_length, n_features)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = si.get_data('PETR4.SA', start_date='20190101', end_date='20201229')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>22.549999</td>\n",
       "      <td>24.200001</td>\n",
       "      <td>22.280001</td>\n",
       "      <td>24.059999</td>\n",
       "      <td>23.284786</td>\n",
       "      <td>104534800.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>23.959999</td>\n",
       "      <td>24.820000</td>\n",
       "      <td>23.799999</td>\n",
       "      <td>24.650000</td>\n",
       "      <td>23.855778</td>\n",
       "      <td>95206400.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>24.850000</td>\n",
       "      <td>24.940001</td>\n",
       "      <td>24.469999</td>\n",
       "      <td>24.719999</td>\n",
       "      <td>23.923521</td>\n",
       "      <td>72119800.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>24.850000</td>\n",
       "      <td>25.920000</td>\n",
       "      <td>24.700001</td>\n",
       "      <td>25.110001</td>\n",
       "      <td>24.300957</td>\n",
       "      <td>121711900.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>25.400000</td>\n",
       "      <td>25.420000</td>\n",
       "      <td>24.770000</td>\n",
       "      <td>24.959999</td>\n",
       "      <td>24.155787</td>\n",
       "      <td>68761800.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-18</th>\n",
       "      <td>28.100000</td>\n",
       "      <td>28.320000</td>\n",
       "      <td>28.049999</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>27351400.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-21</th>\n",
       "      <td>27.190001</td>\n",
       "      <td>27.490000</td>\n",
       "      <td>26.520000</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>27.020000</td>\n",
       "      <td>99988800.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-22</th>\n",
       "      <td>27.200001</td>\n",
       "      <td>27.469999</td>\n",
       "      <td>27.049999</td>\n",
       "      <td>27.280001</td>\n",
       "      <td>27.280001</td>\n",
       "      <td>46513200.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-23</th>\n",
       "      <td>27.430000</td>\n",
       "      <td>28.250000</td>\n",
       "      <td>27.350000</td>\n",
       "      <td>27.950001</td>\n",
       "      <td>27.950001</td>\n",
       "      <td>49038900.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-28</th>\n",
       "      <td>28.360001</td>\n",
       "      <td>28.520000</td>\n",
       "      <td>28.200001</td>\n",
       "      <td>28.280001</td>\n",
       "      <td>28.280001</td>\n",
       "      <td>26661400.0</td>\n",
       "      <td>PETR4.SA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 open       high        low      close   adjclose  \\\n",
       "2019-01-02  22.549999  24.200001  22.280001  24.059999  23.284786   \n",
       "2019-01-03  23.959999  24.820000  23.799999  24.650000  23.855778   \n",
       "2019-01-04  24.850000  24.940001  24.469999  24.719999  23.923521   \n",
       "2019-01-07  24.850000  25.920000  24.700001  25.110001  24.300957   \n",
       "2019-01-08  25.400000  25.420000  24.770000  24.959999  24.155787   \n",
       "...               ...        ...        ...        ...        ...   \n",
       "2020-12-18  28.100000  28.320000  28.049999  28.250000  28.250000   \n",
       "2020-12-21  27.190001  27.490000  26.520000  27.020000  27.020000   \n",
       "2020-12-22  27.200001  27.469999  27.049999  27.280001  27.280001   \n",
       "2020-12-23  27.430000  28.250000  27.350000  27.950001  27.950001   \n",
       "2020-12-28  28.360001  28.520000  28.200001  28.280001  28.280001   \n",
       "\n",
       "                 volume    ticker  \n",
       "2019-01-02  104534800.0  PETR4.SA  \n",
       "2019-01-03   95206400.0  PETR4.SA  \n",
       "2019-01-04   72119800.0  PETR4.SA  \n",
       "2019-01-07  121711900.0  PETR4.SA  \n",
       "2019-01-08   68761800.0  PETR4.SA  \n",
       "...                 ...       ...  \n",
       "2020-12-18   27351400.0  PETR4.SA  \n",
       "2020-12-21   99988800.0  PETR4.SA  \n",
       "2020-12-22   46513200.0  PETR4.SA  \n",
       "2020-12-23   49038900.0  PETR4.SA  \n",
       "2020-12-28   26661400.0  PETR4.SA  \n",
       "\n",
       "[494 rows x 7 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_data(df, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, len(FEATURE_COLUMNS), loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name + \".h5\"), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01685, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01685 to 0.01154, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01154 to 0.01056, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01056\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.01056 to 0.00998, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00998 to 0.00995, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00995 to 0.00990, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00990\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00990 to 0.00984, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00984\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00984\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00984 to 0.00977, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00977\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00977 to 0.00962, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00962 to 0.00955, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00955\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00955\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00955\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00955\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00955\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00955 to 0.00948, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00948 to 0.00941, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00941\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00941\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00941 to 0.00938, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00938\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00938 to 0.00927, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00927\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00927\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00927\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00927 to 0.00924, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00924\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00924\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00924 to 0.00922, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00922 to 0.00905, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00905\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00905\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00905\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00905\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00905\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00905\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.00905 to 0.00878, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00878 to 0.00878, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.00878 to 0.00878, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00878\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.00878 to 0.00869, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00869\n",
      "\n",
      "Epoch 00150: val_loss improved from 0.00869 to 0.00852, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00852\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00852 to 0.00849, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.00849\n",
      "\n",
      "Epoch 00232: val_loss improved from 0.00849 to 0.00848, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.00848\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.00848\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.00848\n",
      "\n",
      "Epoch 00236: val_loss improved from 0.00848 to 0.00845, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00237: val_loss improved from 0.00845 to 0.00831, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.00831\n",
      "\n",
      "Epoch 00253: val_loss improved from 0.00831 to 0.00821, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00254: val_loss improved from 0.00821 to 0.00820, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00255: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.00820\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.00820 to 0.00816, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.00816\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.00816\n",
      "\n",
      "Epoch 00260: val_loss improved from 0.00816 to 0.00811, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.00811\n",
      "\n",
      "Epoch 00265: val_loss improved from 0.00811 to 0.00799, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00266: val_loss improved from 0.00799 to 0.00770, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.00770\n",
      "\n",
      "Epoch 00356: val_loss improved from 0.00770 to 0.00755, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.00755\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.00755\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.00755\n",
      "\n",
      "Epoch 00360: val_loss improved from 0.00755 to 0.00720, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.00720\n",
      "\n",
      "Epoch 00369: val_loss improved from 0.00720 to 0.00609, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.00609\n",
      "\n",
      "Epoch 00394: val_loss improved from 0.00609 to 0.00580, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00395: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.00580\n",
      "\n",
      "Epoch 00408: val_loss improved from 0.00580 to 0.00522, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00409: val_loss improved from 0.00522 to 0.00438, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.00438\n",
      "\n",
      "Epoch 00417: val_loss improved from 0.00438 to 0.00425, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00418: val_loss improved from 0.00425 to 0.00373, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.00373\n",
      "\n",
      "Epoch 00424: val_loss improved from 0.00373 to 0.00322, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.00322 to 0.00291, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.00291\n",
      "\n",
      "Epoch 00432: val_loss improved from 0.00291 to 0.00280, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00433: val_loss improved from 0.00280 to 0.00251, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.00251\n",
      "\n",
      "Epoch 00475: val_loss improved from 0.00251 to 0.00238, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.00238\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.00238 to 0.00232, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.00232\n",
      "\n",
      "Epoch 00488: val_loss improved from 0.00232 to 0.00223, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00489: val_loss improved from 0.00223 to 0.00217, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.00217\n",
      "\n",
      "Epoch 00494: val_loss improved from 0.00217 to 0.00199, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.00199\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.00199\n",
      "\n",
      "Epoch 00497: val_loss improved from 0.00199 to 0.00191, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.00191\n",
      "\n",
      "Epoch 00499: val_loss improved from 0.00191 to 0.00175, saving model to results\\2020-12-29_AMZN-sh-1-sc-1-sbd-0-huber_loss-adam-LSTM-seq-50-step-15-layers-2-units-256.h5\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.00175\n",
      "Tempo total 815.523431301117\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# train the model and save the weights whenever we see \n",
    "# a new optimal model using ModelCheckpoint\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=0, \n",
    "                    use_multiprocessing=True)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Tempo total', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph(test_df):\n",
    "    \"\"\"\n",
    "    This function plots true close price along with predicted close price\n",
    "    with blue and red colors respectively\n",
    "    \"\"\"\n",
    "    plt.plot(test_df[f'true_adjclose_{LOOKUP_STEP}'], c='b')\n",
    "    plt.plot(test_df[f'adjclose_{LOOKUP_STEP}'], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(model, data):\n",
    "    \"\"\"\n",
    "    This function takes the `model` and `data` dict to \n",
    "    construct a final dataframe that includes the features along \n",
    "    with true and predicted prices of the testing dataset\n",
    "    \"\"\"\n",
    "    # if predicted future price is higher than the current, \n",
    "    # then calculate the true future price minus the current price, to get the buy profit\n",
    "    buy_profit  = lambda current, true_future, pred_future: true_future - current if pred_future > current else 0\n",
    "    # if the predicted future price is lower than the current price,\n",
    "    # then subtract the true future price from the current price\n",
    "    sell_profit = lambda current, true_future, pred_future: current - true_future if pred_future < current else 0\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    # perform prediction and get prices\n",
    "    y_pred = model.predict(X_test)\n",
    "    if SCALE:\n",
    "        y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "        y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    test_df = data[\"test_df\"]\n",
    "    # add predicted future prices to the dataframe\n",
    "    test_df[f\"adjclose_{LOOKUP_STEP}\"] = y_pred\n",
    "    # add true future prices to the dataframe\n",
    "    test_df[f\"true_adjclose_{LOOKUP_STEP}\"] = y_test\n",
    "    # sort the dataframe by date\n",
    "    test_df.sort_index(inplace=True)\n",
    "    final_df = test_df\n",
    "    # add the buy profit column\n",
    "    final_df[\"buy_profit\"] = list(map(buy_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    # add the sell profit column\n",
    "    final_df[\"sell_profit\"] = list(map(sell_profit, \n",
    "                                    final_df[\"adjclose\"], \n",
    "                                    final_df[f\"adjclose_{LOOKUP_STEP}\"], \n",
    "                                    final_df[f\"true_adjclose_{LOOKUP_STEP}\"])\n",
    "                                    # since we don't have profit for last sequence, add 0's\n",
    "                                    )\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][-N_STEPS:]\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    if SCALE:\n",
    "        predicted_price = data[\"column_scaler\"][\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    else:\n",
    "        predicted_price = prediction[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal model weights from results folder\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"], verbose=0)\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "if SCALE:\n",
    "    mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform([[mae]])[0][0]\n",
    "else:\n",
    "    mean_absolute_error = mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final dataframe for the testing set\n",
    "final_df = get_final_df(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict the future price\n",
    "future_price = predict(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.347075"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the accuracy by counting the number of positive profits\n",
    "accuracy_score = (len(final_df[final_df['sell_profit'] > 0]) + len(final_df[final_df['buy_profit'] > 0])) / len(final_df)\n",
    "# calculating total buy & sell profit\n",
    "total_buy_profit  = final_df[\"buy_profit\"].sum()\n",
    "total_sell_profit = final_df[\"sell_profit\"].sum()\n",
    "# total profit by adding sell & buy together\n",
    "total_profit = total_buy_profit + total_sell_profit\n",
    "# dividing total profit by number of testing samples (number of trades)\n",
    "profit_per_trade = total_profit / len(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future price after 15 days is 11.87$\n",
      "huber_loss loss: 0.001745138899423182\n",
      "Mean Absolute Error: 12.202229005629905\n",
      "Accuracy score: 0.9397590361445783\n",
      "Total buy profit: 242.20491886138916\n",
      "Total sell profit: 204.7201919555664\n",
      "Total profit: 446.92511081695557\n",
      "Profit per trade: 5.38463988936091\n"
     ]
    }
   ],
   "source": [
    "# printing metrics\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(f\"{LOSS} loss:\", loss)\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "print(\"Accuracy score:\", accuracy_score)\n",
    "print(\"Total buy profit:\", total_buy_profit)\n",
    "print(\"Total sell profit:\", total_sell_profit)\n",
    "print(\"Total profit:\", total_profit)\n",
    "print(\"Profit per trade:\", profit_per_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true/pred prices graph\n",
    "plot_graph(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 open       high        low      close   adjclose  \\\n",
      "2020-09-21  21.290001  21.459999  20.809999  20.900000  20.900000   \n",
      "2020-09-24  20.240000  20.709999  20.059999  20.400000  20.400000   \n",
      "2020-09-25  20.150000  20.260000  19.870001  20.129999  20.129999   \n",
      "2020-10-13  19.910000  20.129999  19.730000  20.129999  20.129999   \n",
      "2020-10-19  19.410000  19.940001  19.250000  19.520000  19.520000   \n",
      "2020-10-20  19.670000  20.270000  19.590000  20.180000  20.180000   \n",
      "2020-10-26  20.330000  20.530001  20.030001  20.250000  20.250000   \n",
      "2020-10-30  19.139999  19.540001  18.870001  18.940001  18.940001   \n",
      "2020-11-05  19.950001  19.990000  19.719999  19.889999  19.889999   \n",
      "2020-11-11  23.350000  23.360001  22.400000  22.879999  22.879999   \n",
      "\n",
      "                 volume    ticker  adjclose_15  true_adjclose_15  buy_profit  \\\n",
      "2020-09-21   73300000.0  PETR4.SA    19.881422         20.129999    0.000000   \n",
      "2020-09-24   76310000.0  PETR4.SA    19.740089         19.340000    0.000000   \n",
      "2020-09-25   40150400.0  PETR4.SA    19.836164         19.520000    0.000000   \n",
      "2020-10-13   55330600.0  PETR4.SA    21.522995         19.719999    0.000000   \n",
      "2020-10-19  107419300.0  PETR4.SA    21.749073         23.080000    2.229073   \n",
      "2020-10-20   68407700.0  PETR4.SA    22.376581         22.879999    2.196581   \n",
      "2020-10-26   58229100.0  PETR4.SA    23.841566         23.690001    3.591566   \n",
      "2020-10-30   64185100.0  PETR4.SA    23.262604         26.219999    4.322603   \n",
      "2020-11-05   40874800.0  PETR4.SA    24.472780         25.889999    4.582781   \n",
      "2020-11-11   93072900.0  PETR4.SA    25.459307         26.639999    2.579308   \n",
      "\n",
      "            sell_profit  \n",
      "2020-09-21     1.018578  \n",
      "2020-09-24     0.659910  \n",
      "2020-09-25     0.293835  \n",
      "2020-10-13    -1.392996  \n",
      "2020-10-19     0.000000  \n",
      "2020-10-20     0.000000  \n",
      "2020-10-26     0.000000  \n",
      "2020-10-30     0.000000  \n",
      "2020-11-05     0.000000  \n",
      "2020-11-11     0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(final_df.tail(10))\n",
    "# save the final dataframe to csv-results folder\n",
    "csv_results_folder = \"csv-results\"\n",
    "if not os.path.isdir(csv_results_folder):\n",
    "    os.mkdir(csv_results_folder)\n",
    "csv_filename = os.path.join(csv_results_folder, model_name + \".csv\")\n",
    "final_df.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
